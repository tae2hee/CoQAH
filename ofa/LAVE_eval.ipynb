{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a4904-9a86-4074-a443-a408104c4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import cv2\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from tasks.mm_tasks.refcoco import RefcocoTask\n",
    "from models.ofa import OFAModel\n",
    "from PIL import Image, ImageOps\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309879db-9145-43e7-a6c2-ba608afb3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = 'vqa_rad_open.tsv'\n",
    "fp = open(os.path.join('vqa_data/', test_set), \"r\")\n",
    "fp.seek(0)\n",
    "images = []\n",
    "gts = []\n",
    "questions = []\n",
    "images2 = []\n",
    "fp.seek(0)\n",
    "while(True):\n",
    "    column_l = fp.readline().rstrip(\"\\n\").split(\"\\t\")\n",
    "    if len(column_l) == 1: break\n",
    "    images.append(column_l[1])\n",
    "    gts.append(column_l[2])\n",
    "    questions.append(column_l[3])\n",
    "    images2.append(column_l[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22800e11-f58c-4659-a10c-04488ac58c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'coqah_5_vqa_rad_open.json'\n",
    "res_dict = json.loads(open(exp_name, 'r').read())\n",
    "res = res_dict[\"res\"]\n",
    "gts = res_dict[\"gts\"]\n",
    "\n",
    "replaced_gts = [re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", x.lower().replace(\"x-ray\", \"xray\").replace(\"x ray\", \"xray\").replace(\"radiography\", \"xray\").replace(\"radiograph\", \"xray\").replace(\"cxr\", \"chest xray\").replace(\"pa view\", \"pa\").replace(\"right side\", \"right\").replace(\"left side\", \"left\")) for x in gts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafda069-05e7-42c5-a067-6b58198f0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ChatGPT class\n",
    "import openai\n",
    "\n",
    "class BasicChatGPT:\n",
    "    def __init__(self, openai_api_key, max_tokens=100):\n",
    "        openai.api_key = openai_api_key\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages = []\n",
    "\n",
    "    def call(self):\n",
    "        response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-4\",\n",
    "          messages=self.messages,\n",
    "          max_tokens=self.max_tokens,\n",
    "          seed = 123,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def prompt(self, text):\n",
    "        # Update the messages so far\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text,\n",
    "        })\n",
    "\n",
    "        # Call ChatGPT\n",
    "        response = self.call()\n",
    "\n",
    "        # Save the returned message\n",
    "        message = response[\"choices\"][0][\"message\"]\n",
    "        self.messages.append(message)\n",
    "\n",
    "        return message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985712d-dceb-463c-98e3-7ae36e7e20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_PROMPT = \"\"\" \n",
    "You are given a question, a set of gold-standard reference answers written by experts, and a candidate answer. Please rate the accuracy of the candidate answer for the question considering the reference answers. Use a scale of 1-3, with 1 indicating an incorrect or irrelevant answer, 2 indicating an ambiguous or incomplete answer, and 3 indicating a correct answer. Give the rationale before rating. THIS IS VERY IMPORTANT: A binary question should only be answered with 'yes' or 'no', otherwise the candidate answer is incorrect.\n",
    "\n",
    "Question: 'What is the color of the car?'\n",
    "Reference answers: 'red', 'red', 'red', 'red', 'scarlet'\n",
    "Candidate answer: 'pink'\n",
    "Output: The candidate answer is incorrect because the car is 'red' and not 'pink'. So rating = 1\n",
    "\n",
    "Question: 'What is the animal on the left?'\n",
    "Reference answers: 'elephant', 'giraffe', 'giraffe', 'giraffe', 'giraffe'\n",
    "Candidate answer: 'giraffe'\n",
    "Output: The candidate answer is correct because most of the reference answers (4 out of 5) indicate the animal on the left is a giraffe. So rating = 3\n",
    "\n",
    "Question: 'Whatâ€™s the weather like?'\n",
    "Reference answers: 'bright', 'bright and sunny', 'clear', 'sunny', 'sunny', 'sunny'\n",
    "Candidate answer: 'cloudy'\n",
    "Output: The candidate answer is incorrect because the weather is 'bright' and 'sunny', not cloudy. So rating = 1\n",
    "\n",
    "Question: 'What are the people in the picture doing?'\n",
    "Reference answers: 'sitting', 'sitting', 'sitting', 'sitting'\n",
    "Candidate answer: 'they are resting'\n",
    "Output: The candidate answer is ambiguous because, while it is common that people who are sitting are resting, it is not always the case. So rating = 2\n",
    "\n",
    "Question: 'What color are the base tiles?'\n",
    "Reference answers: 'beige', 'beige', 'beige', 'brown', 'brown', 'tan', 'tan', 'tan', 'tan', 'ten'\n",
    "Candidate answer: 'brown'\n",
    "Output: The candidate answer is correct because the reference answers include 'brown' and other similar colors such as 'tan' or 'beige'. So rating = 3\n",
    "\n",
    "Question: 'How many people are in the picture?'\n",
    "Reference answers: 'four', 'three', 'three', 'three', 'two', 'two'\n",
    "Candidate answer: 'a few'\n",
    "Output: The candidate answer is incomplete because 'a few' is less specific than the numerical reference answers. So rating = 2\n",
    "\n",
    "Question: 'What type of fruit is in the picture?'\n",
    "Reference answers: 'apple'\n",
    "Candidate answer: 'fruit'\n",
    "Output: The candidate answer is incorrect because it does not specify the type of fruit. So rating = 1\n",
    "\n",
    "Question: 'What type of sculpture is this?'\n",
    "Reference answers: 'Horse statue.'\n",
    "Candidate answer: 'horse'\n",
    "Output: The candidate answer is correct because 'horse' is equivalent to 'horse statue' in this context. So rating = 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a409b-32b2-4e98-9764-3248fd1e9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = exp_name.replace('', 'lave_')\n",
    "\n",
    "if os.path.isfile(save_file):\n",
    "    lave_res = json.loads(open(save_file, 'r').read())\n",
    "    starting_idx = len(lave_res)\n",
    "else:\n",
    "    lave_res = []\n",
    "    starting_idx = 0\n",
    "\n",
    "for idx in range(starting_idx, len(gts)):\n",
    "    lave_res_dict = {}\n",
    "    test_prompt = f\"\\n\\nQuestion: '{questions[idx]}'\\nReference answers: '{replaced_gts[idx]}'\\nCandidate answer: '{res[idx]}'\\nOutput: \"\n",
    "    prompt = STARTING_PROMPT + test_prompt\n",
    "    openai_api_key = \"<YOUR_OPENAI_API_KEY>\"\n",
    "    chatgpt = BasicChatGPT(openai_api_key)\n",
    "    response = chatgpt.prompt(prompt)\n",
    "    rate_idx = response.find('rating = ') + len('rating = ')\n",
    "    rating = response[rate_idx:rate_idx+1]\n",
    "    score = (int(rating)-1)/2\n",
    "    lave_res_dict[\"id\"] = idx\n",
    "    lave_res_dict[\"question\"] = questions[idx]\n",
    "    lave_res_dict[\"gt\"] = replaced_gts[idx]\n",
    "    lave_res_dict[\"res\"] = res[idx]\n",
    "    lave_res_dict[\"response\"] = response\n",
    "    lave_res_dict[\"rating\"] = rating\n",
    "    lave_res_dict[\"score\"] = score\n",
    "    lave_res.append(lave_res_dict)\n",
    "    with open(save_file, 'w') as f:\n",
    "        json.dump(lave_res, f)\n",
    "    print(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofa",
   "language": "python",
   "name": "ofa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
