{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf6410-7849-46f4-b84a-a765705fa137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import cv2\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from tasks.mm_tasks.refcoco import RefcocoTask\n",
    "from models.ofa import OFAModel\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4aaeb-38eb-4d1a-b107-b68b5f90068d",
   "metadata": {},
   "source": [
    "# Load VQA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2a169-5f0d-438a-9c87-4f8c49657967",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.register_task('refcoco', RefcocoTask)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_fp16 = False\n",
    "\n",
    "model_path = 'OFA/checkpoint_best.pt'\n",
    "parser = options.get_generation_parser()\n",
    "input_args = [\"\", \"--task=refcoco\", \"--beam=10\", f\"--path={model_path}\", \"--bpe-dir=utils/BPE\", \"--no-repeat-ngram-size=3\", \"--patch-image-size=384\"]\n",
    "args = options.parse_args_and_arch(parser, input_args)\n",
    "cfg = convert_namespace_to_omegaconf(args)\n",
    "\n",
    "# Load pretrained ckpt & config\n",
    "task = tasks.setup_task(cfg.task)\n",
    "vqa_models, cfg = checkpoint_utils.load_model_ensemble(\n",
    "    utils.split_paths(cfg.common_eval.path),\n",
    "    task=task\n",
    ")\n",
    "# Move models to GPU\n",
    "for model in vqa_models:\n",
    "    model.eval()\n",
    "    if use_fp16:\n",
    "        model.half()\n",
    "    if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n",
    "        model.cuda()\n",
    "    model.prepare_for_inference_(cfg)\n",
    "\n",
    "# Initialize generator\n",
    "vqa_generator = task.build_generator(vqa_models, cfg.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5902535-9b0b-41c6-9df8-9788821104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform\n",
    "from torchvision import transforms\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "patch_resize_transform = transforms.Compose([\n",
    "    lambda image: image.convert(\"RGB\"),\n",
    "    transforms.Resize((task.cfg.patch_image_size, task.cfg.patch_image_size), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.RandomHorizontalFlip(p=0)\n",
    "])\n",
    "\n",
    "# Text preprocess\n",
    "bos_item = torch.LongTensor([task.src_dict.bos()])\n",
    "eos_item = torch.LongTensor([task.src_dict.eos()])\n",
    "pad_idx = task.src_dict.pad()\n",
    "\n",
    "\n",
    "def get_symbols_to_strip_from_output(generator):\n",
    "    if hasattr(generator, \"symbols_to_strip_from_output\"):\n",
    "        return generator.symbols_to_strip_from_output\n",
    "    else:\n",
    "        return {generator.bos, generator.eos}\n",
    "\n",
    "\n",
    "def decode_fn(x, tgt_dict, bpe, generator, tokenizer=None):\n",
    "    x = tgt_dict.string(x.int().cpu(), extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator))\n",
    "    token_result = []\n",
    "    bin_result = []\n",
    "    img_result = []\n",
    "    for token in x.strip().split():\n",
    "        if token.startswith('<bin_'):\n",
    "            bin_result.append(token)\n",
    "        elif token.startswith('<code_'):\n",
    "            img_result.append(token)\n",
    "        else:\n",
    "            if bpe is not None:\n",
    "                token = bpe.decode('{}'.format(token))\n",
    "            if tokenizer is not None:\n",
    "                token = tokenizer.decode(token)\n",
    "            if token.startswith(' ') or len(token_result) == 0:\n",
    "                token_result.append(token.strip())\n",
    "            else:\n",
    "                token_result[-1] += token\n",
    "\n",
    "    return ' '.join(token_result), ' '.join(bin_result), ' '.join(img_result)\n",
    "\n",
    "\n",
    "def coord2bin(coords, w_resize_ratio, h_resize_ratio):\n",
    "    coord_list = [float(coord) for coord in coords.strip().split()]\n",
    "    bin_list = []\n",
    "    bin_list += [\"<bin_{}>\".format(int(round(coord_list[0] * w_resize_ratio / task.cfg.max_image_size * (task.cfg.num_bins - 1))))]\n",
    "    bin_list += [\"<bin_{}>\".format(int(round(coord_list[1] * h_resize_ratio / task.cfg.max_image_size * (task.cfg.num_bins - 1))))]\n",
    "    bin_list += [\"<bin_{}>\".format(int(round(coord_list[2] * w_resize_ratio / task.cfg.max_image_size * (task.cfg.num_bins - 1))))]\n",
    "    bin_list += [\"<bin_{}>\".format(int(round(coord_list[3] * h_resize_ratio / task.cfg.max_image_size * (task.cfg.num_bins - 1))))]\n",
    "    return ' '.join(bin_list)\n",
    "\n",
    "\n",
    "def bin2coord(bins, w_resize_ratio, h_resize_ratio):\n",
    "    bin_list = [int(bin[5:-1]) for bin in bins.strip().split()]\n",
    "    coord_list = []\n",
    "    coord_list += [bin_list[0] / (task.cfg.num_bins - 1) * task.cfg.max_image_size / w_resize_ratio]\n",
    "    coord_list += [bin_list[1] / (task.cfg.num_bins - 1) * task.cfg.max_image_size / h_resize_ratio]\n",
    "    coord_list += [bin_list[2] / (task.cfg.num_bins - 1) * task.cfg.max_image_size / w_resize_ratio]\n",
    "    coord_list += [bin_list[3] / (task.cfg.num_bins - 1) * task.cfg.max_image_size / h_resize_ratio]\n",
    "    return coord_list\n",
    "\n",
    "\n",
    "def encode_text(text, length=None, append_bos=False, append_eos=False):\n",
    "    line = [\n",
    "      task.bpe.encode(' {}'.format(word.strip())) \n",
    "      if not word.startswith('<code_') and not word.startswith('<bin_') else word\n",
    "      for word in text.strip().split()\n",
    "    ]\n",
    "    line = ' '.join(line)\n",
    "    s = task.tgt_dict.encode_line(\n",
    "        line=line,\n",
    "        add_if_not_exist=False,\n",
    "        append_eos=False\n",
    "    ).long()\n",
    "    if length is not None:\n",
    "        s = s[:length]\n",
    "    if append_bos:\n",
    "        s = torch.cat([bos_item, s])\n",
    "    if append_eos:\n",
    "        s = torch.cat([s, eos_item])\n",
    "    return s\n",
    "\n",
    "def construct_sample(image: Image, instruction: str, image2: Image = None):\n",
    "    if image2 is None:\n",
    "        patch_image2 = None\n",
    "    else:\n",
    "        patch_image2 = patch_resize_transform(image2).unsqueeze(0)\n",
    "    \n",
    "    patch_image = patch_resize_transform(image).unsqueeze(0)\n",
    "    patch_mask = torch.tensor([True])\n",
    "\n",
    "    instruction = encode_text(' {}'.format(instruction.lower().strip()), append_bos=True, append_eos=True).unsqueeze(0)\n",
    "    instruction_length = torch.LongTensor([s.ne(pad_idx).long().sum() for s in instruction])\n",
    "    sample = {\n",
    "        \"id\":np.array(['42']),\n",
    "        \"net_input\": {\n",
    "            \"src_tokens\": instruction,\n",
    "            \"src_lengths\": instruction_length,\n",
    "            \"patch_images\": patch_image,\n",
    "            \"patch_images_2\": patch_image2,\n",
    "            \"patch_masks\": patch_mask,\n",
    "        }\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "def construct_sample_wo_image(instruction: str):\n",
    "    patch_mask = torch.tensor([True])\n",
    "    instruction = encode_text(' {}'.format(instruction.lower().strip()), append_bos=True, append_eos=True).unsqueeze(0)\n",
    "    instruction_length = torch.LongTensor([s.ne(pad_idx).long().sum() for s in instruction])\n",
    "    sample = {\n",
    "        \"id\":np.array(['42']),\n",
    "        \"net_input\": {\n",
    "            \"src_tokens\": instruction,\n",
    "            \"src_lengths\": instruction_length,\n",
    "            \"patch_masks\": patch_mask,\n",
    "        }\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "# Function to turn FP32 to FP16\n",
    "def apply_half(t):\n",
    "    if t.dtype is torch.float32:\n",
    "        return t.to(dtype=torch.half)\n",
    "    return t\n",
    "\n",
    "def infer_ofa(img, question):\n",
    "    sample = construct_sample(img, question)\n",
    "    sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
    "    sample = utils.apply_to_sample(apply_half, sample) if use_fp16 else sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hypos = task.inference_step(vqa_generator, vqa_models, sample)\n",
    "        tokens, bins, imgs = decode_fn(hypos[0][0][\"tokens\"], task.tgt_dict, task.bpe, vqa_generator)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c60fa-d81a-4f56-9c09-742f6628e289",
   "metadata": {},
   "source": [
    "## Setting for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db2ab4-0244-4120-9f59-cdeb1cfa07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic ChatGPT class\n",
    "import openai\n",
    "\n",
    "class BasicChatGPT:\n",
    "    def __init__(self, openai_api_key, max_tokens=100):\n",
    "        openai.api_key = openai_api_key\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages = []\n",
    "\n",
    "    def call(self):\n",
    "        response = openai.ChatCompletion.create(\n",
    "          model=\"gpt-4\",\n",
    "          messages=self.messages,\n",
    "          max_tokens=self.max_tokens,\n",
    "          seed = 123,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def prompt(self, text):\n",
    "        # Update the messages so far\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text,\n",
    "        })\n",
    "\n",
    "        # Call ChatGPT\n",
    "        response = self.call()\n",
    "\n",
    "        # Save the returned message\n",
    "        message = response[\"choices\"][0][\"message\"]\n",
    "        self.messages.append(message)\n",
    "\n",
    "        return message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc407c81-15c9-436e-baed-1558f3a3548d",
   "metadata": {},
   "source": [
    "## Functions for CoQAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba4758-912e-470c-90a1-5cbc252c9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_PROMPT = \"\"\" \n",
    "But you can't access the image and I can access the image.\n",
    "You can ask me questions with these forms.\n",
    "The question should be in []\n",
    "If you can answer the above question, stop asking and give me an answer within 1 word.\n",
    "The answer should be in {}\n",
    "[is there a <ENTITY> ?]\n",
    "[what abnormalities are seen in this image?]\n",
    "[where is the <ENTITY> ?]\n",
    "[what level is the <ENTITY> ?]\n",
    "[what type is the <ENTITY> ?]\n",
    "[which view is this image taken?]\n",
    "\n",
    "<ENTITY> :  [pleural effusion or atelectasis or cardiomegaly or enlargement of the cardiac silhouette or edema or hernia or vascular congestion or hilar congestion or pneumothorax or heart failure or lung opacity or pneumonia or tortuosity of the descending aorta or scoliosis or gastric distention or hypoxemia or hypertensive heart disease or hematoma or tortuosity of the thoracic aorta or contusion or emphysema or granuloma or calcification or pleural thickening or thymoma or blunting of the costophrenic angle or consolidation or fracture or pneumomediastinum or air collection]\n",
    "\n",
    "Ask me the next question after I answer you.\n",
    "Ask me questions carefully considering existence presupposition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1b625-19e0-4341-ac1b-acb6433e8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAKET_ERROR = \"The question should be in []\"\n",
    "FORMAT_ERROR = \"Unsupported format or unsupported option. choose the most similar option, even if itâ€™s not totally the same.\"\n",
    "EXISTENCE_ERROR = \"there is no \"\n",
    "\n",
    "ENTITY = [\"pleural effusion\", \"atelectasis\", \"cardiomegaly\", \"enlargement of cardiac silhouette\", \"edema\", \"hernia\", \"vascular congestion\", \"hilar congestion\", \"pneumothorax\", \"heart failure\", \"lung opacity\", \"pneumonia\", \"tortuosity of descending aorta\", \"scoliosis\", \"gastric distention\", \"hypoxemia\", \"hypertensive heart disease\", \"hematoma\", \"tortuosity of thoracic aorta\", \"contusion\", \"emphysema\", \"granuloma\", \"calcification\", \"pleural thickening\", \"thymoma\", \"blunting of costophrenic angle\", \"consolidation\", \"fracture\", \"pneumomediastinum\", \"air collection\"]\n",
    "\n",
    "question_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cfc245-9205-46de-b749-f5749a1f9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_why(chatgpt):\n",
    "    prompt = \"Why?\"\n",
    "    prompt = \"User: \" + prompt\n",
    "    response = chatgpt.prompt(prompt)\n",
    "    return response\n",
    "\n",
    "def guess_answer(chatgpt):\n",
    "    prompt = \"\"\"You have used all the chances to ask questions. Now, guess a answer anyway.\n",
    "    Give me an answer within 1 word.\n",
    "    The answer should be in {}\n",
    "    \"\"\"\n",
    "    prompt = \"User: \" + prompt\n",
    "    response = chatgpt.prompt(prompt)\n",
    "    return response\n",
    "\n",
    "def post_process_answer(answer):\n",
    "    answer = answer.lower().replace(\".\",\"\")\n",
    "    return answer\n",
    "\n",
    "def check_answer_format(answer, chatgpt): \n",
    "    if answer == 'unknown':\n",
    "        prompt = \"\"\" You can't say 'unknown'. guess an answer anyway.\n",
    "        The answer should be in {}\n",
    "        \"\"\"\n",
    "        print(prompt)\n",
    "        prompt = \"User: \" + prompt\n",
    "        response = chatgpt.prompt(prompt)\n",
    "        abidx = response.find('{')\n",
    "        aeidx = response.find('}')\n",
    "        if abidx != -1 or aeidx != -1:\n",
    "            response = response[abidx+1:aeidx]\n",
    "        return False, response\n",
    "    \n",
    "    return True, None\n",
    "\n",
    "def check_format(response):\n",
    "    entities = response.lower().replace(\"?\",\"\").replace(\" a \",\" \").replace(\" an \",\" \").replace(\" the \",\" \").replace(\"is there\",\"\").replace(\"where is\",\"\").replace(\"what level is\",\"\").replace(\"what type is\",\"\").strip()\n",
    "    if \"what abnormalities are seen in this image\" in entities or \"which view is this image taken\" in entities:\n",
    "        return True, \"\", None\n",
    "    if (entities not in ENTITY):\n",
    "        return False, entities , entities\n",
    "    return True, entities, None\n",
    "\n",
    "def check_existence(entity):\n",
    "    ex_q = f\"is there a {entity}?\"\n",
    "    ex_a = infer_ofa(img, ex_q)\n",
    "    if \"yes\" in ex_a:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def handling_response(response, hist, rep):\n",
    "    abidx = response.find('{')\n",
    "    aeidx = response.find('}')\n",
    "    if abidx != -1 or aeidx != -1:\n",
    "        a_gpt = response[abidx+1:aeidx]\n",
    "        return True, a_gpt, \"DONE!\", hist\n",
    "    \n",
    "    qbidx = response.find('[')\n",
    "    qeidx = response.find(']')\n",
    "    if qbidx == -1 or qeidx == -1:\n",
    "        return False, None, BRAKET_ERROR, hist\n",
    "    q_gpt = response[qbidx+1:qeidx]\n",
    "    q_gpt = q_gpt.lower()\n",
    "    correct_format, entity, unsup_e = check_format(q_gpt)\n",
    "    if not correct_format:\n",
    "        return False, None, unsup_e + \": \" + FORMAT_ERROR, hist\n",
    "    hist[f\"E{str(rep)}\"] = entity\n",
    "    if len(entity) > 0:\n",
    "        existence = check_existence(entity)\n",
    "        if not existence:\n",
    "            return False, None, EXISTENCE_ERROR + entity, hist\n",
    "\n",
    "    hist[f\"P{str(rep)}\"] = q_gpt\n",
    "    feedback = infer_ofa(img, q_gpt)\n",
    "    \n",
    "    return False, None, feedback, hist\n",
    "    \n",
    "def chatgpt_dialogue(prompt):\n",
    "    memo = {\"history\": []}\n",
    "    openai_api_key = \"<YOUR_OPENAI_API_KEY>\n",
    "    chatgpt = BasicChatGPT(openai_api_key)\n",
    "    done = False\n",
    "    rep = 0 \n",
    "    while(not done and rep < question_limit):\n",
    "        hist = {}\n",
    "        prompt = \"User: \" + prompt\n",
    "        response = chatgpt.prompt(prompt)\n",
    "        hist[f\"Q{str(rep)}\"] = response\n",
    "        done, answer, prompt, hist = handling_response(response, hist, rep)\n",
    "        hist[f\"A{str(rep)}\"] = prompt\n",
    "        memo[\"history\"].append(hist)\n",
    "        rep += 1\n",
    "    if not done:\n",
    "        response = guess_answer(chatgpt)\n",
    "        done, answer, prompt, hist = handling_response(response, hist, rep)\n",
    "        memo[\"guess\"] = answer\n",
    "    memo[\"raw_answer\"] = answer\n",
    "    processed_answer = post_process_answer(answer)\n",
    "    memo[\"answer\"] = processed_answer\n",
    "    valid, re_answer = check_answer_format(processed_answer, chatgpt)\n",
    "    reason = ask_why(chatgpt)\n",
    "    memo[\"reason\"] = reason\n",
    "    if valid:\n",
    "        return processed_answer, memo\n",
    "    else:\n",
    "        memo[\"raw_re_answer\"] = re_answer\n",
    "        processed_re_answer = post_process_answer(re_answer)\n",
    "        memo[\"re_answer\"] = processed_re_answer\n",
    "        return processed_re_answer, memo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16104672-fb96-4b47-a8e9-f2b37ea11472",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb279e-6203-4859-a99f-995befa2a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = 'slake_all_closed.tsv'\n",
    "fp = open(os.path.join('vqa_data/', test_set), \"r\")\n",
    "fp.seek(0)\n",
    "images = []\n",
    "gts = []\n",
    "questions = []\n",
    "images2 = []\n",
    "fp.seek(0)\n",
    "while(True):\n",
    "    column_l = fp.readline().rstrip(\"\\n\").split(\"\\t\")\n",
    "    if len(column_l) == 1: break\n",
    "    images.append(column_l[1])\n",
    "    gts.append(column_l[2])\n",
    "    questions.append(column_l[3])\n",
    "    images2.append(column_l[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d5371-0413-4b37-bebb-0118c6fb8bc4",
   "metadata": {},
   "source": [
    "## Run CoQAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba1499-c605-4374-b386-70be0990c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from sklearn.metrics import *\n",
    "import json\n",
    "\n",
    "save_file = 'coqah_5_slake_all_closed.json'\n",
    "base_save_file = save_file.replace('coqah','base')\n",
    "\n",
    "if os.path.isfile(save_file):\n",
    "    res_dict = json.loads(open(save_file, 'r').read())\n",
    "    base_dict = json.loads(open(base_save_file, 'r').read())\n",
    "    starting_idx = res_dict[\"id\"][-1]+1\n",
    "else:\n",
    "    res_dict = {\"id\": [], \"res\": [], \"gts\": [], \"memo\": []}\n",
    "    base_dict = {\"id\": [], \"res\": [], \"gts\": []}\n",
    "    starting_idx = 0\n",
    "    \n",
    "for idx in range(starting_idx, len(gts)):\n",
    "    img = Image.open(BytesIO(base64.urlsafe_b64decode(images[idx])))\n",
    "    gt = gts[idx]\n",
    "    question = questions[idx]\n",
    "     \n",
    "    qprompt = f\"Your goal is to answer this question:\\n{question}\\n\"\n",
    "    sprompt = qprompt + STARTING_PROMPT\n",
    "    try:\n",
    "        answer, memo = chatgpt_dialogue(sprompt)\n",
    "    except:\n",
    "        print(\"wating for time limit\")\n",
    "        time.sleep(60)\n",
    "        answer, memo = chatgpt_dialogue(sprompt)\n",
    "    \n",
    "    res_dict[\"id\"].append(idx)\n",
    "    res_dict[\"res\"].append(answer)\n",
    "    res_dict[\"gts\"].append(gt)\n",
    "    res_dict[\"memo\"].append(memo)\n",
    "    acc = accuracy_score(res_dict[\"gts\"], res_dict[\"res\"])\n",
    "    with open(save_file, 'w') as f:\n",
    "        json.dump(res_dict, f)\n",
    "    print(idx)\n",
    "    print(question)\n",
    "    print(\"gt: \", gt)\n",
    "    print(\"pred: \", answer)\n",
    "    print(\"AFA ACC: \",acc)\n",
    "    # baseline_answer = infer_ofa(img, question)\n",
    "    # base_dict[\"id\"].append(idx)\n",
    "    # base_dict[\"res\"].append(baseline_answer)\n",
    "    # base_dict[\"gts\"].append(gt)\n",
    "    # base_acc = accuracy_score(base_dict[\"gts\"], base_dict[\"res\"])\n",
    "    # with open(base_save_file, 'w') as f:\n",
    "    #     json.dump(base_dict, f)\n",
    "    # print(\"Baseline: \", baseline_answer)\n",
    "    # print(\"Baseline ACC: \", base_acc)\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofa",
   "language": "python",
   "name": "ofa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
